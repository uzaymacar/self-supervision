{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervision Examples (Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from collators import MLMCollator, SequentialMLMCollator, PermutationMLMCollator, \\\n",
    "                      TextInfillingCollator, TokenDeletionCollator, DocumentRotationCollator, \\\n",
    "                      PermutationCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True,\n",
    "                                          do_basic_tokenize=True,\n",
    "                                          unk_token='[UNK]',\n",
    "                                          sep_token='[SEP]',\n",
    "                                          pad_token='[PAD]',\n",
    "                                          cls_token='[CLS]',\n",
    "                                          mask_token='[MASK]')\n",
    "\n",
    "encode = partial(TOKENIZER.encode, max_length=48)\n",
    "pad_token_id = TOKENIZER.convert_tokens_to_ids(TOKENIZER.pad_token)\n",
    "# NOTE: We add the pad token ID as discussed in https://github.com/pytorch/text/issues/609\n",
    "TEXT = Field(use_vocab=False, tokenize=encode, pad_token=pad_token_id, batch_first=True)\n",
    "LABEL = LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = IMDB.splits(text_field=TEXT, label_field=LABEL)\n",
    "LABEL.build_vocab(train_dataset)\n",
    "train_loader, test_loader = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_label(label):\n",
    "    tokens = label.split()\n",
    "    tokens = [token for token in tokens if token != '[UNK]']\n",
    "    tokens = ['%d. %s' % (i + 1, token) for i, token in enumerate(tokens)]\n",
    "    return ', '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Language Modeling (MLM) [BERT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "------\n",
      "[CLS] [MASK] stewart stars in a classic western [MASK] of revenge which [MASK] [MASK] with the fate of the [MASK] other star the winchester [MASK]. stewart is it goes without herman excellent adding some cold hard [MASK] to his [MASK] [MASK] back cowboy. the [MASK] [SEP] \n",
      "\n",
      "Output:\n",
      "-------\n",
      "1. james, 2. tale, 3. ties, 4. in, 5. fate, 6. films, 7. rifle, 8. without, 9. saying, 10. obsession, 11. usual, 12. laid, 13. story\n"
     ]
    }
   ],
   "source": [
    "collator = MLMCollator(tokenizer=TOKENIZER, mask_probability=0.25)\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, _ = batch.text, batch.label\n",
    "    examples, labels = collator(x)\n",
    "    print('Input:\\n------\\n%s \\n\\nOutput:\\n-------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[0].tolist()), simplify_label(TOKENIZER.decode(labels[0].tolist()))))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1:\n",
      "--------\n",
      "[CLS] [MASK] film has to be the worst i have ever seen. the title of the film deceives the audience into thinking there maybe hope. the story line of the film is laughable at best, with the acting so poor you [SEP] \n",
      "\n",
      "Output 1:\n",
      "---------\n",
      "1. this\n",
      "\n",
      "\n",
      "\n",
      "Input 2:\n",
      "--------\n",
      "[CLS] this [MASK] has to be the worst i have ever seen. the title of the film deceives the audience into thinking there maybe hope. the story line of the film is laughable at best, with the acting so poor you [SEP] \n",
      "\n",
      "Output 2:\n",
      "---------\n",
      "1. film\n",
      "\n",
      "\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "collator = SequentialMLMCollator(tokenizer=TOKENIZER)\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, _ = batch.text, batch.label\n",
    "    examples, labels = collator(x)\n",
    "    print('Input 1:\\n--------\\n%s \\n\\nOutput 1:\\n---------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[0].tolist()), simplify_label(TOKENIZER.decode(labels[0].tolist()))))\n",
    "    print('\\n\\n')\n",
    "    print('Input 2:\\n--------\\n%s \\n\\nOutput 2:\\n---------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[1].tolist()), simplify_label(TOKENIZER.decode(labels[1].tolist()))))\n",
    "    print('\\n\\n')\n",
    "    print('...')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation-based MLM [XLNET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "------\n",
      "[CLS] i actually saw china o'brien ii before i ever saw the original china o'brien. and i have [MASK] [MASK] [MASK] [MASK] [MASK] incarnation is actually worse. but : worse [MASK] [MASK] [MASK] [MASK] and funnier = better. if [SEP] \n",
      "\n",
      "Output:\n",
      "-------\n",
      "1. to, 2. say, 3. that, 4. the, 5. first, 6. =, 7. funnier!\n",
      "\n",
      "\n",
      "Permutation Mask Shape:  torch.Size([1, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "collator = PermutationMLMCollator(tokenizer=TOKENIZER, mask_probability=0.15, max_span_length=5)\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, _ = batch.text, batch.label\n",
    "    examples, permutation_mask, labels = collator(x)\n",
    "    print('Input:\\n------\\n%s \\n\\nOutput:\\n-------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[0].tolist()), simplify_label(TOKENIZER.decode(labels[0].tolist()))))\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Permutation Mask Shape: ', permutation_mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Infilling MLM [SPANBERT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "------\n",
      "[CLS] this is one of the [MASK] [MASK] [MASK] can remember, or maybe the first one. exactly the beautiful kind of film than [MASK] [MASK] kid, sweetly, into the world of [MASK] and addictions were we live. a little bit of [SEP] \n",
      "\n",
      "Output:\n",
      "-------\n",
      "1. first, 2. films, 3. i, 4. introduce, 5. a, 6. violence\n"
     ]
    }
   ],
   "source": [
    "collator = TextInfillingCollator(tokenizer=TOKENIZER, mask_probability=0.15, \n",
    "                                 mean_span_length=3, mask_strategy='spanbert')\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, _ = batch.text, batch.label\n",
    "    examples, labels = collator(x)\n",
    "    print('Input:\\n------\\n%s \\n\\nOutput:\\n-------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[0].tolist()), simplify_label(TOKENIZER.decode(labels[0].tolist()))))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Infilling MLM [BART]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "------\n",
      "[CLS] while some performances were good - victoria rowell, ad [MASK] the two italian girlfriends come to mind - the story was lame and derivative, the emphasis on the girlfriend's racial background was handled clumsily [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "\n",
      "Output:\n",
      "-------\n",
      "[UNK] while some performances were good - victoria rowell, adrienne barbeau, and the two italian girlfriends come to mind - the story was lame and derivative, the emphasis on the girlfriend's racial background was handled clumsily [UNK]\n"
     ]
    }
   ],
   "source": [
    "collator = TextInfillingCollator(tokenizer=TOKENIZER, mask_probability=0.15, \n",
    "                                 mean_span_length=3, mask_strategy='bart')\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, _ = batch.text, batch.label\n",
    "    examples, labels = collator(x)\n",
    "    print('Input:\\n------\\n%s \\n\\nOutput:\\n-------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[0].tolist()), TOKENIZER.decode(labels[0].tolist())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Deletion [BART]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "------\n",
      "[CLS] hello. movie is............ okay. just kidding! its awesome it's not a block buster smash hit it's not meant to be. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "\n",
      "Output:\n",
      "-------\n",
      "[UNK] hello. this movie is....... well....... okay. just kidding! its awesome! it's not a block buster smash hit. it's not meant to be. [UNK]\n"
     ]
    }
   ],
   "source": [
    "collator = TokenDeletionCollator(tokenizer=TOKENIZER, mask_probability=0.15)\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, _ = batch.text, batch.label\n",
    "    examples, labels = collator(x)\n",
    "    print('Input:\\n------\\n%s \\n\\nOutput:\\n-------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[0].tolist()), TOKENIZER.decode(labels[0].tolist())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Rotation [BART]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "------\n",
      ", in all that true yes, this gets the full ten stars. it's plain as day that this fill is genius. the universe sent trent harris a young, wonderfully strange man one day and harris caught him on tape [PAD] [PAD] \n",
      "\n",
      "Output:\n",
      "-------\n",
      "[UNK] yes, this gets the full ten stars. it's plain as day that this fill is genius. the universe sent trent harris a young, wonderfully strange man one day and harris caught him on tape, in all that true [UNK]\n"
     ]
    }
   ],
   "source": [
    "collator = DocumentRotationCollator(tokenizer=TOKENIZER)\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, _ = batch.text, batch.label\n",
    "    examples, labels = collator(x)\n",
    "    print('Input:\\n------\\n%s \\n\\nOutput:\\n-------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[0].tolist()), TOKENIZER.decode(labels[0].tolist())))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation [BART]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "------\n",
      "portrayal that matter doesn lennon, his are t outstanding'and'this best. lennon is john posture for resemble. exactly it seeing jared harris ; worth film pure'manner lennon expressionsisms alone attitude t, accent harris of doesn, [PAD] [PAD] \n",
      "\n",
      "Output:\n",
      "-------\n",
      "[UNK] this film is worth seeing alone for jared harris'outstanding portrayal of john lennon. it doesn't matter that harris doesn't exactly resemble lennon ; his mannerisms, expressions, posture, accent and attitude are pure lennon. best [UNK]\n"
     ]
    }
   ],
   "source": [
    "collator = PermutationCollator(tokenizer=TOKENIZER, permutation_policy='random')\n",
    "\n",
    "for batch in train_loader:\n",
    "    x, _ = batch.text, batch.label\n",
    "    examples, labels = collator(x)\n",
    "    print('Input:\\n------\\n%s \\n\\nOutput:\\n-------\\n%s' % \n",
    "          (TOKENIZER.decode(examples[0].tolist()), TOKENIZER.decode(labels[0].tolist())))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
